{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Criteria:\n",
    "**Choosing the best attribute to split** the data at each node. This is done by maximizing the information gain or minimizing impurity.\n",
    "\n",
    "\n",
    "- **Entropy**\n",
    "$$H(T) = -Σ p(i|t) \\log₂(p(i|t))$$\n",
    "where $p(i|t)$ is the proportion of samples belonging to class $i$ at node $t$.\n",
    "\n",
    "> Entropy comes from information theory and measures uncertainty or randomness in a dataset (using logarithms)\n",
    "\n",
    "- **Gini Index (same directionas Entropy)**\n",
    "$$G(T) = 1 - Σ [p(i|t)]²$$\n",
    "\n",
    "> ***Gini Impurity measures how often a randomly chosen sample would be incorrectly labelled***\n",
    "\n",
    "- **Misclassification Error**\n",
    "$$E(T) = 1 - max[p(i|t)]$$\n",
    "\n",
    "- **Information Gain**\n",
    "Information gain is the difference in entropy before and after the split:\n",
    "\n",
    "$$IG(T, a) = H(T) - Σ (|Tv| / |T|) * H(Tv)$$\n",
    "where $a$ is the attribute, $T$ is the parent node, $Tv$ are the child nodes, and $|T|$ is the number of samples at node $T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Criteria (Regression):\n",
    "**Variance reduction instead of class impurity**\n",
    "\n",
    "$$ Var(S) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **COMPUTATION**\n",
    "- **Entropy is more complex since it makes use of logarithms and consequently, the calculation of the Gini Index will be faster**\n",
    "- entropy tends to create more balanced trees\n",
    "- Gini is intended for continuous attributes and Entropy is for attributes that occur in classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For pure node: Entropy = 0, Gini = 0\n",
    "- For impure node: Entropy > 0, Gini > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **STOPPING CRITERIA**\n",
    "\n",
    "1. Maximum depth\n",
    "2. Minimum number of samples per leaf/split\n",
    "3. Minimum impurity reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **COMPLEXITY**\n",
    "\n",
    "$$O(n d logn)$$\n",
    "\n",
    "where,\n",
    "\n",
    "- $n$ is the number of samples\n",
    "- $d$ is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (IMPORTANT) FEATURE IMPORTANCE USING GINI\n",
    "\n",
    "- Most common method\n",
    "- We know that for each split - how much Gini impurity is reduced\n",
    "- Total reduction in impurity that each feature contributes is averaged across all trees in the forest\n",
    "- **Higher the Gini reduction, higher the feature importance**\n",
    "\n",
    "\n",
    "**Disadvantage**\n",
    "\n",
    "- Can favour features with many values over those with fewer values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
