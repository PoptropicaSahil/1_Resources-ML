{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations\n",
    "$$z = Xw + b$$\n",
    "$$y_{pred} = \\sigma(z) = 1 / (1 + e^{-z}) = P(Y=1|X)$$ \n",
    "$$y_{output} = y > 0.5 (threshold) $$\n",
    "\n",
    "$remember \\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z) (1-\\sigma(z))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function (Log loss/BCE/CE)\n",
    "$$L(w,b) = -1/m * [Σ(y * log(y_{pred}) + (1-y) * log(1-y_{pred}))]$$\n",
    "\n",
    "**Remember as Log-istic Regression** -- Inside the log we have the regression ouptuts i.e. predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "**Start with $L$ for a single sample**:\n",
    "$L = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$\n",
    "\n",
    "**Gradient descent using chain rule**\n",
    "$\\frac{\\partial L}{\\partial \\hat{w}} = \\frac{\\partial L}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z} \\times \\frac{\\partial z}{\\partial w}$\n",
    "\n",
    "\n",
    "#### Step 1: $\\frac{\\partial L}{\\partial \\hat{y}}$\n",
    "$\\frac{\\partial L}{\\partial \\hat{y}} = -[\\frac{y}{\\hat{y}} - \\frac{1-y}{1-\\hat{y}}]$\n",
    "\n",
    "#### Step 2: $\\frac{\\partial \\hat{y}}{\\partial z}$\n",
    "$\\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y}(1 - \\hat{y})$ \n",
    "\n",
    "\n",
    "#### Step 3: $\\frac{\\partial L}{\\partial z}$ \n",
    "$\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z}$\n",
    "\n",
    "$= (-[\\frac{y}{\\hat{y}} - \\frac{1-y}{1-\\hat{y}}]) \\cdot (\\hat{y}(1 - \\hat{y}))$\n",
    "\n",
    "$= -y(1 - \\hat{y}) + (1-y)\\hat{y}$\n",
    "\n",
    "$= \\hat{y} - y$\n",
    "\n",
    "#### Step 4: $\\frac{\\partial z}{\\partial w}$ and $\\frac{\\partial z}{\\partial b}$\n",
    "$\\frac{\\partial z}{\\partial w} = x$\n",
    "$\\frac{\\partial z}{\\partial b} = 1$\n",
    "\n",
    "#### Step 5: $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ (using chain rule)\n",
    "$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w} = (\\hat{y} - y) \\cdot x$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b} = \\hat{y} - y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Final Gradients (for multiple samples)\n",
    "\n",
    "For multiple samples, we take the average of these gradients:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n [(y_{pred_i} - y_i) \\cdot x_i]$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (y_{pred_i} - y_i)$\n",
    "\n",
    "In vector notation:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = \\frac{1}{n} X^T (\\hat{y} - y)$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\sum (\\hat{y} - y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple explanation\n",
    "- $X$ is a matrix of shape $(m, n)$\n",
    "- $y_{pred}$ and $y$ are vectors of shape $(m,)$.\n",
    "- The operation $X^T \\cdot (y_{pred} - y)$ implicitly performs the summation for $\\frac{\\partial L}{\\partial w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$∂L/∂w = (1/m) * X^T * (y_{pred} - y)$$ \n",
    "$$∂L/∂b = (1/m) * Σ(y_{pred} - y)$$\n",
    "\n",
    "Update rule:\n",
    "$$w = w - α * ∂J/∂w$$\n",
    "$$b = b - α * ∂J/∂b$$\n",
    "- $α$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the beauty of log-loss\n",
    "- How $\\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z) (1-\\sigma(z))$\n",
    "- How the partial derivatives are same as linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iterations):\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(z)\n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(z)\n",
    "        return (y_pred > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (100, 3), y shape = (100,)\n",
      "X elements = [[ 0.3285971  -0.79619855  1.40312383]\n",
      " [-1.54779275  1.1667303   1.14772265]\n",
      " [ 0.13010933  0.43142236 -0.86831976]\n",
      " [ 0.02715937 -1.51873922 -0.81556143]\n",
      " [-0.22363745 -2.63109611  0.04686695]], y elements = [1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "np.random.seed(110)\n",
    "X = np.random.randn(100, 3)\n",
    "y = (X[:,0] > 0).astype(int)  # Binary classification randomly\n",
    "print(f\"X shape = {X.shape}, y shape = {y.shape}\")\n",
    "print(f\"X elements = {X[:5]}, y elements = {y[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.23932621342690608),\n",
       " array([ 5.39647907, -0.29443978, -0.17058492]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model = LogisticRegression(learning_rate=0.1, n_iterations=1000)\n",
    "model.fit(X, y)\n",
    "model.bias, model.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.21863165e-01, 1.08215967e-04, 6.18668473e-01, 6.20939656e-01,\n",
       "        3.36378157e-01]),\n",
       " array([1, 0, 1, 1, 0]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.dot(X[:5], model.weights) + model.bias\n",
    "y_pred = model.sigmoid(z)\n",
    "y_pred, (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
