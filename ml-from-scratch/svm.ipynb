{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Formulation of SVMs:\n",
    "\n",
    "## Linear SVM for Separable Data\n",
    "\n",
    "\n",
    "**Intro** \\\n",
    "Given a training dataset of $n$ points of the form $(xᵢ, yᵢ)$ \\\n",
    "where \n",
    "- $xᵢ ∈ ℝᵈ$\n",
    "- $yᵢ ∈ \\{-1, 1\\}$ \n",
    "\n",
    "We want to **find the hyperplane that maximizes the margin between the two classes**\n",
    "\n",
    "\n",
    "The hyperplane is defined as $w · x - b = 0$\n",
    "where \n",
    "- $w$ is the normal vector to the hyperplane\n",
    "\n",
    "We want to maximize the margin $2 / ||w||$ subject to the constraint:\n",
    "$yᵢ(w · xᵢ - b) ≥ 1$ for $i = 1, ..., n$\n",
    "\n",
    "This can be formulated as a quadratic optimization problem:\n",
    "$$minimize ~(1/2)||w||²$$\n",
    "subject to $yᵢ(w · xᵢ - b) ≥ 1$ for $i = 1, ..., n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin SVM (for Non-Separable Data)\n",
    "We introduce slack variables $ξᵢ ≥ 0$ to allow for misclassification:\n",
    "$$minimize ~ (1/2)||w||² + C Σᵢ ξᵢ$$\n",
    "subject to $yᵢ(w · xᵢ - b) ≥ 1 - ξᵢ$ and $ξᵢ ≥ 0$ for $i = 1, ..., n$\n",
    "Here, $C > 0$ is a regularization parameter that controls the trade-off between maximizing the margin and minimizing the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Form\n",
    "\n",
    "Using Lagrange multipliers, we can derive the dual form:\n",
    "$$maximize~ Σᵢ αᵢ - (1/2)Σᵢ Σⱼ αᵢαⱼyᵢyⱼ(xᵢ · xⱼ)$$\n",
    "subject to $0 ≤ αᵢ ≤ C$ and $Σᵢ αᵢyᵢ = 0$\n",
    "The optimal $w$ is given by $Σᵢ αᵢyᵢxᵢ$\n",
    "\n",
    "## Kernel Trick:\n",
    "\n",
    "For non-linearly separable data, we can use the kernel trick. We replace the dot product $xᵢ · xⱼ$ with a kernel function $K(xᵢ, xⱼ)$.\n",
    "Common kernels include:\n",
    "\n",
    "- Linear: $K(xᵢ, xⱼ) = xᵢ · xⱼ$\n",
    "- Polynomial: $K(xᵢ, xⱼ) = (γxᵢ · xⱼ + r)ᵈ$\n",
    "- RBF (Gaussian): $K(xᵢ, xⱼ) = exp(-γ||xᵢ - xⱼ||²)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
