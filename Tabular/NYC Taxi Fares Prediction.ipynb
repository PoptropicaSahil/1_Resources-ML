{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43/1203895254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %matplotlib inline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 6\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "tqdm.pandas(desc='Progress')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(df, start_lat, end_lat, start_lng, end_lng, prefix):\n",
    "    \"\"\"\n",
    "    calculates haversine distance between 2 sets of GPS coordinates in df\n",
    "    \"\"\"\n",
    "    R = 6371  #radius of earth in kilometers\n",
    "       \n",
    "    phi1 = np.radians(df[start_lat])\n",
    "    phi2 = np.radians(df[end_lat])\n",
    "    \n",
    "    delta_phi = np.radians(df[end_lat]-df[start_lat])\n",
    "    delta_lambda = np.radians(df[end_lng]-df[start_lng])\n",
    "    \n",
    "        \n",
    "    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (R * c) #in kilometers\n",
    "    df[prefix+'distance_km'] = d\n",
    "\n",
    "def add_datepart(df, col, prefix):\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr: df[prefix + n] = getattr(df[col].dt, n.lower())\n",
    "    df[prefix + 'Elapsed'] = df[col].astype(np.int64) // 10 ** 9\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "def reject_outliers(data, m = 2.):\n",
    "    d = np.abs(data - np.median(data))\n",
    "    mdev = np.median(d)\n",
    "    s = d/(mdev if mdev else 1.)\n",
    "    return s<m\n",
    "\n",
    "def parse_gps(df, prefix):\n",
    "    lat = prefix + '_latitude'\n",
    "    lon = prefix + '_longitude'\n",
    "    df[prefix + '_x'] = np.cos(df[lat]) * np.cos(df[lon])\n",
    "    df[prefix + '_y'] = np.cos(df[lat]) * np.sin(df[lon]) \n",
    "    df[prefix + '_z'] = np.sin(df[lat])\n",
    "    df.drop([lat, lon], axis=1, inplace=True)\n",
    "    \n",
    "def prepare_dataset(df):\n",
    "    df['pickup_datetime'] = pd.to_datetime(df.pickup_datetime, infer_datetime_format=True)\n",
    "    add_datepart(df, 'pickup_datetime', 'pickup')\n",
    "    haversine_distance(df, 'pickup_latitude', 'dropoff_latitude', 'pickup_longitude', 'dropoff_longitude', '')\n",
    "    parse_gps(df, 'pickup')\n",
    "    parse_gps(df, 'dropoff')\n",
    "    df.dropna(inplace=True)\n",
    "    y = np.log(df.fare_amount)\n",
    "    df.drop(['key', 'fare_amount'], axis=1, inplace=True)\n",
    "    \n",
    "    return df, y\n",
    "\n",
    "def split_features(df):\n",
    "    catf = ['pickupYear', 'pickupMonth', 'pickupWeek', 'pickupDay', 'pickupDayofweek', \n",
    "            'pickupDayofyear', 'pickupHour', 'pickupMinute', 'pickupSecond', 'pickupIs_month_end',\n",
    "            'pickupIs_month_start', 'pickupIs_quarter_end', 'pickupIs_quarter_start',\n",
    "            'pickupIs_year_end', 'pickupIs_year_start']\n",
    "\n",
    "    numf = [col for col in df.columns if col not in catf]\n",
    "    for c in catf: \n",
    "        df[c] = df[c].astype('category').cat.as_ordered()\n",
    "        df[c] = df[c].cat.codes+1\n",
    "    \n",
    "    return catf, numf\n",
    "\n",
    "def numericalize(df):\n",
    "    df[name] = col.cat.codes+1\n",
    "\n",
    "def split_dataset(df, y): return train_test_split(df, y, test_size=0.25, random_state=42)\n",
    "\n",
    "def inv_y(y): return np.exp(y)\n",
    "\n",
    "def get_numf_scaler(train): return preprocessing.StandardScaler().fit(train)\n",
    "\n",
    "def scale_numf(df, num, scaler):\n",
    "    cols = numf\n",
    "    index = df.index\n",
    "    scaled = scaler.transform(df[numf])\n",
    "    scaled = pd.DataFrame(scaled, columns=cols, index=index)\n",
    "    return pd.concat([scaled, df.drop(numf, axis=1)], axis=1)\n",
    "\n",
    "class RegressionColumnarDataset(data.Dataset):\n",
    "    def __init__(self, df, cats, y):\n",
    "        self.dfcats = df[cats]\n",
    "        self.dfconts = df.drop(cats, axis=1)\n",
    "        \n",
    "        self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(np.int64)\n",
    "        self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32)\n",
    "        self.y = y.values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.conts[idx], self.y[idx]]\n",
    "    \n",
    "def rmse(targ, y_pred):\n",
    "    return np.sqrt(mean_squared_error(inv_y(y_pred), inv_y(targ))) #.detach().numpy()\n",
    "\n",
    "def emb_init(x):\n",
    "    x = x.weight.data\n",
    "    sc = 2/(x.size(1)+1)\n",
    "    x.uniform_(-sc,sc)\n",
    "\n",
    "class MixedInputModel(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops, y_range, use_bn=True):\n",
    "        super().__init__()\n",
    "        for i,(c,s) in enumerate(emb_szs): assert c > 1, f\"cardinality must be >=2, got emb_szs[{i}]: ({c},{s})\"\n",
    "        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n",
    "        for emb in self.embs: emb_init(emb)\n",
    "        n_emb = sum(e.embedding_dim for e in self.embs)\n",
    "        self.n_emb, self.n_cont=n_emb, n_cont\n",
    "        \n",
    "        szs = [n_emb+n_cont] + szs\n",
    "        self.lins = nn.ModuleList([nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(sz) for sz in szs[1:]])\n",
    "        for o in self.lins: nn.init.kaiming_normal_(o.weight.data)\n",
    "        self.outp = nn.Linear(szs[-1], out_sz)\n",
    "        nn.init.kaiming_normal_(self.outp.weight.data)\n",
    "\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        self.use_bn,self.y_range = use_bn,y_range\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x2 = self.bn(x_cont)\n",
    "            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n",
    "        for l,d,b in zip(self.lins, self.drops, self.bns):\n",
    "            x = F.relu(l(x))\n",
    "            if self.use_bn: x = b(x)\n",
    "            x = d(x)\n",
    "        x = self.outp(x)\n",
    "        if self.y_range:\n",
    "            x = torch.sigmoid(x)\n",
    "            x = x*(self.y_range[1] - self.y_range[0])\n",
    "            x = x+self.y_range[0]\n",
    "        return x.squeeze()\n",
    "\n",
    "def fit(model, train_dl, val_dl, loss_fn, opt, scheduler, epochs=3):\n",
    "    num_batch = len(train_dl)\n",
    "    for epoch in tnrange(epochs):      \n",
    "        y_true_train = list()\n",
    "        y_pred_train = list()\n",
    "        total_loss_train = 0          \n",
    "        \n",
    "        t = tqdm_notebook(iter(train_dl), leave=False, total=num_batch)\n",
    "        for cat, cont, y in t:\n",
    "            cat = cat.cuda()\n",
    "            cont = cont.cuda()\n",
    "            y = y.cuda()\n",
    "            \n",
    "            t.set_description(f'Epoch {epoch}')\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            pred = model(cat, cont)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            lr[epoch].append(opt.param_groups[0]['lr'])\n",
    "            tloss[epoch].append(loss.item())\n",
    "            scheduler.step()\n",
    "            opt.step()\n",
    "            \n",
    "            t.set_postfix(loss=loss.item())\n",
    "            \n",
    "            y_true_train += list(y.cpu().data.numpy())\n",
    "            y_pred_train += list(pred.cpu().data.numpy())\n",
    "            total_loss_train += loss.item()\n",
    "            \n",
    "        train_acc = rmse(y_true_train, y_pred_train)\n",
    "        train_loss = total_loss_train/len(train_dl)\n",
    "        \n",
    "        if val_dl:\n",
    "            y_true_val = list()\n",
    "            y_pred_val = list()\n",
    "            total_loss_val = 0\n",
    "            for cat, cont, y in tqdm_notebook(val_dl, leave=False):\n",
    "                cat = cat.cuda()\n",
    "                cont = cont.cuda()\n",
    "                y = y.cuda()\n",
    "                pred = model(cat, cont)\n",
    "                loss = loss_fn(pred, y)\n",
    "                \n",
    "                y_true_val += list(y.cpu().data.numpy())\n",
    "                y_pred_val += list(pred.cpu().data.numpy())\n",
    "                total_loss_val += loss.item()\n",
    "                vloss[epoch].append(loss.item())\n",
    "            valacc = rmse(y_true_val, y_pred_val)\n",
    "            valloss = total_loss_val/len(valdl)\n",
    "            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f} | val_loss: {valloss:.4f} val_rmse: {valacc:.4f}')\n",
    "        else:\n",
    "            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f}')\n",
    "    \n",
    "    return lr, tloss, vloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tha data is a half-million random sample from the 55M original training set from the [New York City Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data) Kaggle's challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='data/taxi/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "names = ['key','fare_amount','pickup_datetime','pickup_longitude',\n",
    "         'pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']\n",
    "df = pd.read_csv(f'{PATH}traindf.csv', header=None, names=names)\n",
    "\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count.describe()\n",
    "\n",
    "df.passenger_count.quantile([.85, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fare_amount.describe()\n",
    "\n",
    "df.fare_amount.quantile([.85, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[(df.fare_amount > 0) & (df.passenger_count < 6) & (df.fare_amount < 53),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y = prepare_dataset(df)\n",
    "\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = y.hist(bins=20, figsize=(8,6))\n",
    "_ = ax.set_xlabel(\"Ride Value (EUR)\")\n",
    "_ = ax.set_ylabel(\"# Rides\")\n",
    "_ = ax.set_title('Ditribution of Ride Values (USD)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catf, numf = split_features(df)\n",
    "\n",
    "len(catf)\n",
    "catf\n",
    "\n",
    "len(numf)\n",
    "numf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_dataset(df, y)\n",
    "\n",
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = get_numf_scaler(X_train[numf])\n",
    "\n",
    "X_train_sc = scale_numf(X_train, numf, scaler)\n",
    "X_train_sc.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sc = scale_numf(X_test, numf, scaler)\n",
    "\n",
    "X_train_sc.shape\n",
    "X_test_sc.shape\n",
    "X_test_sc.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining pytorch datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds = RegressionColumnarDataset(X_train_sc, catf, y_train)\n",
    "valds = RegressionColumnarDataset(X_test_sc, catf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43/3389876250.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m           'num_workers': 8}\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtraindl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mvaldl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "params = {'batch_size': 128,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 8}\n",
    "\n",
    "traindl = data.DataLoader(trainds, **params)\n",
    "valdl = data.DataLoader(valds, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model and related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = (0, y_train.max()*1.2)\n",
    "y_range\n",
    "\n",
    "cat_sz = [(c, df[c].max()+1) for c in catf]\n",
    "cat_sz\n",
    "\n",
    "emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MixedInputModel(emb_szs=emb_szs, \n",
    "                    n_cont=len(df.columns)-len(catf), \n",
    "                    emb_drop=0.04, \n",
    "                    out_sz=1, \n",
    "                    szs=[1000,500,250], \n",
    "                    drops=[0.001,0.01,0.01], \n",
    "                    y_range=y_range).to(device)\n",
    "\n",
    "opt = optim.Adam(m.parameters(), 1e-2)\n",
    "lr_cosine = lr_scheduler.CosineAnnealingLR(opt, 1000)\n",
    "\n",
    "lr = defaultdict(list)\n",
    "tloss = defaultdict(list)\n",
    "vloss = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, tloss, vloss = fit(model=m, train_dl=traindl, val_dl=valdl, loss_fn=F.mse_loss, opt=opt, scheduler=lr_cosine, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(lr[0])\n",
    "_ = plt.title('Learning Rate Cosine Annealing over Train Batches Iterations (Epoch 0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [np.mean(tloss[el]) for el in tloss]\n",
    "v = [np.mean(vloss[el]) for el in vloss]\n",
    "p = pd.DataFrame({'Train Loss': t, 'Validation Loss': v, 'Epochs': range(10)})\n",
    "\n",
    "_ = p.plot(x='Epochs', y=['Train Loss', 'Validation Loss'], \n",
    "           title='Train and Validation Loss over Epochs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
